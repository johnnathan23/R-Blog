[
  {
    "path": "posts/2024-08-06-writing-101-key-summary-of-style-the-basics-of-clarity-and-grace/",
    "title": "Writing 101: Key Summary of 'Style: The Basics of Clarity and Grace'",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Jonathan Choo",
        "url": {}
      }
    ],
    "date": "2024-08-06",
    "categories": [],
    "contents": "\r\nPart 1: Overview and Clarity\r\nOverview of “Style: The Basics of Clarity and Grace”\r\n“Style: The Basics of Clarity and Grace” is structured around several key principles of effective writing:\r\nClarity\r\nCohesion and Coherence\r\nEmphasis\r\nConcision\r\nShape (overall structure)\r\nElegance\r\nThe book argues that good writing is not just about following grammar rules, but about considering the reader’s experience and understanding. It emphasizes that clear, graceful writing stems from clear thinking and a genuine desire to communicate effectively.\r\nImproving Clarity in Writing\r\nUse Strong Subjects and Verbs:\r\nIdentify the main characters (or concepts) in your sentence.\r\nMake these characters the subjects of your sentences.\r\nUse strong, active verbs to show what these characters are doing.\r\nExample:\r\nWeak: There was a decision made by the committee to implement the new policy.\r\nStrong: The committee decided to implement the new policy.\r\nKeep Subjects and Verbs Close:\r\nPlace the subject near the beginning of the sentence.\r\nKeep the verb close to the subject.\r\nAvoid long interruptions between the subject and verb.\r\nExample:\r\nUnclear: The policy, which was controversial due to its potential impact on small businesses and its perceived favoritism towards large corporations, was implemented.\r\nClear: The policy was implemented, despite controversy over its potential impact on small businesses and perceived favoritism towards large corporations.\r\nTurn Nominalizations into Verbs:\r\nIdentify abstract nouns (often ending in -tion, -ment, -ance).\r\nConvert these nouns back into the verbs they come from.\r\nExample:\r\nNominalized: The implementation of the new system resulted in the reduction of errors.\r\nVerbal: Implementing the new system reduced errors.\r\nUse Consistent Subjects:\r\nMaintain a consistent subject across related sentences.\r\nThis helps the reader follow your train of thought more easily.\r\nExample:\r\nInconsistent: The new policy was implemented. It caused controversy among employees. Management defended their decision.\r\nConsistent: Management implemented the new policy. They defended their decision despite controversy among employees.\r\nAvoid Excessive Metadiscourse:\r\nMetadiscourse refers to phrases that comment on the writing itself.\r\nWhile some metadiscourse can be helpful, too much can obscure your main point.\r\nExample:\r\nExcessive: In this essay, I will attempt to argue that, in my opinion, based on the evidence I have examined, it seems that climate change is a significant issue.\r\nImproved: Climate change is a significant issue, as evidenced by rising global temperatures and extreme weather events.\r\nThese principles form the foundation for clear writing. By applying them, you can significantly improve the clarity and directness of your prose.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-08-06T13:13:42+08:00",
    "input_file": "writing-101-key-summary-of-style-the-basics-of-clarity-and-grace.knit.md"
  },
  {
    "path": "posts/2023-02-22-generating-a-word-cloud/",
    "title": "Generating A Word Cloud",
    "description": "This post is about generating a word cloud. Though you can easily generate one using numerous online programs, utilizing R allows for greater flexibility as well as improving your knowledge of the R Language",
    "author": [
      {
        "name": "Jonathan Choo",
        "url": {}
      }
    ],
    "date": "2023-02-22",
    "categories": [],
    "contents": "\r\nTo start, we first need to install the following packages and load it to our R session\r\n\r\n\r\ninstall.packages(\"wordcloud\")\r\nlibrary(wordcloud)\r\ninstall.packages(\"RColorBrewer\")\r\nlibrary(RColorBrewer)\r\ninstall.packages(\"wordcloud2\")\r\nlibrary(wordcloud2)\r\ninstall.packages(\"dplyr\")\r\nlibrary(dplyr)\r\n\r\n\r\nNext, we need to store our text data as a vector so that we can manipulate it easier. To do this, we use the corpus() function from the tm package to create a corpus of our text data\r\n\r\n\r\ninstall.packages(\"tm\")\r\nlibrary(tm)\r\n\r\n\r\nThen, load up your data into your working directory and workspace. Lets call the dataframe as dataframe\r\n\r\n\r\ndataframe <- read.csv(\"your_file.csv\")\r\n\r\n\r\nWhat you want to do next is to store the text data from your dataframe into a vector and create a corpus (let’s call this corpus as docs) out of that vector. In this example, Q19 contains all the text data information that we want to turn into a word cloud\r\n\r\n\r\n#Create a vector containing only the text. \r\ntextdata <- dataframe$Q19\r\n\r\n# Create a corpus  \r\ndocs <- Corpus(VectorSource(textdata))\r\n\r\n\r\nNext, we want to make sure that only important/useful words are included in the corpus. Currently, numbers, punctuations and stopwords (e.g., and, the, a, is) are still included in the corpus. We need to clean these up or they will show up in the word cloud. To do so, we use the tm_map function from the tm package\r\n\r\n\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\n\r\n\r\nAs we are working with bilingual answers (English and Bahasa Malaysia), we also want to remove the BM stopwords. This requires data from the malaytextr package\r\n\r\n\r\ninstall.packages(\"malaytextr\")\r\nlibrary(malaytextr)\r\n\r\n#We store the stopwords as a matrix in an object called stopmalay\r\nstopmalay <- as.matrix(malaytextr::malaystopwords)\r\n\r\n#We then remove the malay stopwords from the earlier document\r\ndocs <- tm_map(docs, removeWords, stopmalay)\r\n\r\n\r\nWhat you want to do as a next step is to have a new dataframe containing each word in your first column and their frequency in the second column.\r\nThis can be done by creating a document term matrix with the TermDocumentMatrix function from the tm package.\r\n\r\n\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\ndf <- data.frame(word = names(words),freq=words)\r\n\r\n\r\nFinally, we generate our word cloud using the wordcloud package\r\n\r\n\r\nset.seed(1234) # for reproducibility \r\n\r\nwordcloud(words = df$word, freq = df$freq, \r\n          min.freq = 1, \r\n          max.words=200, \r\n          random.order=FALSE,\r\n          rot.per=0.35,\r\n          scale=c(2.5,0.25),\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\nSometimes, depending on the size of your corpus, the plot may not show all words. If this is the case, you should play around with the scale=c(2.5,0.25) argument until an acceptable number of information is present in your word cloud.\r\nEagle-eyed individuals will see that we have not used the wordcloud2 package. This package is a bit more fun to use, allowing us to create word clouds in various shapes such as below:\r\n\r\n\r\nwordcloud2(data=df, size = 0.4, shape = 'diamond')\r\n\r\n\r\n\r\nHappy Learning!\r\nJonathan\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-22-generating-a-word-cloud/generating-a-word-cloud_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-06-28T15:22:34+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-09-crashcourse-on-linear-mixed-models/",
    "title": "Crash Course on Linear Mixed Models",
    "description": "This post is intended as a crash course introduction on LMMs based on how I would have liked to learn the basics of LMMs if I were to learn it from scratch. I am by no means an expert on LMMs, and thus, some information given here might be inaccurate, but where possible, i will cite appropriate papers and books to help assist in further reading and learning",
    "author": [
      {
        "name": "Jonathan Choo",
        "url": {}
      }
    ],
    "date": "2022-08-09",
    "categories": [],
    "contents": "\r\nWelcome to a crash course on Linear Mixed Models (LMMs)! This post is based heavily on the introductory tutorial written by Brown (2012), hence, a lot of the explanations here are paraphrased from the original paper, but mixed in with some additional information that may help in a clearer understanding. There are slight variations in the naming conventions of LMMs, you may encounter Linear Mixed Efffect Models, Mixed Models, Multi-level Models, rest assured, these all refer to the same concept that we will be going through today.\r\nBefore we proceed, this course assumes that the reader has:\r\nBasics in Statistics in Psychology\r\nSpecifically Regression Analysis and Analysis of Variance (ANOVA)\r\nBasics of R language\r\nBasics of Data Structure and Organization\r\nIf you are not knowledgeable in these areas, it is highly recommended that you brush up on these skills before attempting this course.\r\nHave everything? Great! Let us start off with introducing what LMEs are\r\nLMEs: The What, the Why, and the How\r\nLMEs: The What\r\nLMEs are essentially a regression analysis on steroids, providing the flexibility of traditional regression analysis, but allowing for more accurate model fitting compared to regular variance analysis like ANOVA. It achieves this flexibility by introducing two types of effects: 1) Fixed Effects and 2) Random Effects - hence the name “mixed model”, as it includes both of these effects, rather than a traditional Fixed-Effects only model like ANOVAs.\r\nLMEs: The Why\r\nPlaceholder\r\nLMEs: The How\r\nUnderstanding Fixed Effects and Random Effects is crucial in running an LME analysis. To understand what these effects are, Freeman (2017) provides an excellent overview along with a beautifully animated visualization on Fixed Effects and Random Effects.\r\nIn simple words, Fixed effects are usually predictors of interest; Random effects (inclusive of both random intercepts and random slopes) are predictors that exerts a varying influence on the outcome. Using the lme4 package, we can create a model that includes both of these effects. The code below exemplifies a typical mixed model\r\n\r\n\r\nlibrary(lme4)\r\n\r\n\r\n\r\nExample_Model <- lmer(Outcome ~  Fixed Effect + (1|Random Effect), data = Example_Data)\r\n\r\nA Random Effect term is expressed as (1|X), it is here where you need to include vital information about random slopes and intercepts. The more random effects you have, and the more complicated the structure is, the more likely your model will become overfitted. Also, these random effects are usually higher-order grouping variables. Meaning, they supersede other experimental variables such as age, SES, Gender. For example, the two most common random effects are subjects and stimuli because these variables groups other experimental variables under them. Importantly, random effects must be a within-subject variable, that is, the variable must be the same in all blocks of your experiment. If a variable is different in each block, it cannot be a random effect. For example, if we wanted to include a by-stimuli random effect, we must ensure that the stimuli used is the same across all blocks. If different blocks use different types of stimuli, you cannot use that stimuli as a random effect. You may consider running an LME on a block-by-block basis, but this restricts your analysis.\r\nSupposing we wanted to include both by-subject and by-stimuli random effects, how would it be modelled in R? Take a look at the code below:\r\n\r\nExample_Model <- lmer(Outcome ~  Fixed Effect + (1|subject) + (1|stimuli), data = Example_Data)\r\n\r\nRandom Intercepts\r\nTo include a random intercept, we include a “1” (i.e. (1|)), indicating the intercept, within the random effect structure. This informs the model to allow for the intercept to vary for each new participant. This is important because as Freeman (2017) suggests, not all participants start off at the same level. Allowing for the intercept to vary for each participant allows for a better fitting model. This goes the same as with the stimuli. Not all stimuli are created equal, hence, allowing each stimuli to be evaluated at their own intercept creates a better fitted model. The intercept is usually the outcome of your model, why? Look at the regression equation below:\r\n\r\n\r\nY = mX + C\r\n\r\n\r\nThe Y here indicates your intercept, which is the outcome of the model. The X here refers to the values of your X variable, whatever that may be. The m refers to the gradient of that particular X variable, and the C refers to the residual error.\r\nSo, if our outcome is RT, our random effect structure with random intercepts is basically asking the model to allow for every participants to “start off” at different RT values. In the case of Freeman (2017), their outcome was salary, hence, their by-subject random intercept tells the model to allow for every subject to start off at different salaries.\r\nRandom Slopes\r\nTo include a random slope, we first need to identify which of our variables of interest could have different effects across subjects. It might be easier to understand this by looking at our regression equation again. Specifically, random slopes are your m value. Your m value determines how “strong” and effect has over the outcome, a high value of m suggests that for every one value change in your X, it would result in a greater increase/decrease in the Y, and vice versa.\r\nOnce we have identified our variable, we can include a random slope in our random effect structure by placing the variable to the left of the pipe as shown in the code below:\r\n\r\nExample_Model <- lmer(Outcome ~  Fixed Effect + \r\n                        (1 + Fixed Effect|subject) +\r\n                        (1 + Fixed Effect|stimuli), data = Example_Data)\r\n\r\nHere, we took the Fixed Effect and included it as a random slope, making the assumption that the effects of the Fixed Effect can change across both subject and stimuli. Of course, this must be theoretically justified, or your model may not make sense (even if significant)\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-08-06T13:13:03+08:00",
    "input_file": "crashcourse-on-linear-mixed-models.knit.md"
  },
  {
    "path": "posts/2022-04-19-k-means-clustering-tutorial-customer-demographic-data/",
    "title": "K-Means Clustering Tutorial: Customer Demographic Data",
    "description": "In this post, we introduce what k-means clustering is, how it works in R and how to interpret\nthe results to provide actionable outcomes for a hypothetical Mall to improve their profits",
    "author": [
      {
        "name": "Jonathan Choo",
        "url": {}
      }
    ],
    "date": "2022-04-19",
    "categories": [],
    "contents": "\r\nWelcome to this short introduction to k-means! First, let me explain\r\nwhat k-means is to give you a better idea on what we will be learning\r\nhere.\r\nK-means clustering is a form of unsupervised machine\r\nlearning clustering algorithm which falls under the\r\n“partitioning clustering” umbrella. Unlike supervised machine learning,\r\nunsupervised ML does not have access to ground truth to compare the\r\noutput of the clustering algorithm to the true labels to evaluate its\r\nperformance. We only want to try to investigate the structure of the\r\ndata by grouping the data points into distinct subgroups that are as\r\nsimilar together as possible. It is a very popular ML clustering\r\nalgorithm, and very simple to implement in any data set. The other type\r\nof clustering is Hierachical clustering, which we will not be touching\r\non here in this post. However, within partitioning clustering, we also\r\nhave another algorithm called Fuzzy C-Means clustering.\r\nThe main difference between these two algorithms is that K-means\r\nclustering assumes that every data point belongs to one single\r\n“cluster”. Fuzzy C-Means clustering, on the other hand, relaxes this\r\nassumption and provides leeway for some data points to be in two or more\r\nclusters. Where you would use K-means clustering for understanding\r\nsimple patterns in the data set (i.e., what are the characteristics of\r\ncertain groups), you would use Fuzzy C-Means clustering when the group\r\nmembership criteria are not as clear cut. In both cases, the objective\r\nis straightforward, group similar data points together to uncover\r\nunderlying patterns.\r\nFor the purposes of this tutorial, we will just focus on K-means\r\nclustering.\r\nFor K-means to perform its function, it looks for a fixed number\r\n(k) of clusters in a dataset. A cluster refers to a collection\r\nof data points aggregated together because of certain similarities. You\r\nfirst provide an initial k value to determine the preliminary\r\nmodel fit. Based on the accuracy of this model, you then increase or\r\ndecrease the number of clusters that the model should output. We will\r\ncover the optimal method to determining the best k value, but\r\nfor now, understand that you provide an arbitrary number of k\r\nand develop your model from that.\r\nUnderstanding Our Data\r\nLet’s begin by first understanding what kind of data we are working\r\nwith. You can download the data from Kaggle\r\n\r\n\r\ndf <- read.csv(\"Mall_Customers.csv\")\r\nhead(df)\r\n\r\n\r\n  CustomerID Gender Age Annual.Income..k.. Spending.Score..1.100.\r\n1          1   Male  19                 15                     39\r\n2          2   Male  21                 15                     81\r\n3          3 Female  20                 16                      6\r\n4          4 Female  23                 16                     77\r\n5          5 Female  31                 17                     40\r\n6          6 Female  22                 17                     76\r\n\r\nHere, we have four usable data columns: Gender, Age, Annual income\r\nand Spending Score. For us to generate meaningful k-means clusters, we\r\nfirst need to ensure that the data we have are numbers, not factors or\r\ncharacters. Because Gender is in characters, we either can filter out\r\nMales and Females into their separate data sets, or, we can include them\r\ninto the analysis by dummy-coding them. We will include them into the\r\nanalysis by dummy coding them\r\n\r\n\r\ndf$Gender[df$Gender == \"Male\"] <- 1\r\ndf$Gender[df$Gender == \"Female\"] <- 2\r\ndf$Gender <- as.numeric(df$Gender)\r\n\r\n\r\n\r\nBecause CustomerID is meaningless, merely as a way to differentiate\r\ncustomers, we should remove it from the dataframe to ensure only the\r\nrelevant data is used for clustering\r\n\r\n\r\ndf <- df[,2:5]\r\nhead(df)\r\n\r\n\r\n  Gender Age Annual.Income..k.. Spending.Score..1.100.\r\n1      1  19                 15                     39\r\n2      1  21                 15                     81\r\n3      2  20                 16                      6\r\n4      2  23                 16                     77\r\n5      2  31                 17                     40\r\n6      2  22                 17                     76\r\n\r\nOur dataframe is now prepared for clustering analysis. Before we\r\nproceed, let us try to understand what we are trying to learn from this\r\nanalysis. In the dataframe, you can see the age, annual income and\r\nspending score attached to each customer. One insightful data that we\r\ncan try to garner from this analysis is to determine the characteristics\r\nof subgroups. For example:\r\nWould older adults with a higher annual income have a higher\r\nspending score or would they spend less?\r\nWould there be any difference when we compare between genders?\r\nThese are some questions that we can answer based on the clustering\r\nanalysis that we will implement. Now, let us load in the required\r\npackages and run the clustering analysis\r\nImplementing K-Means\r\nClustering Algorithm\r\n\r\n\r\nlibrary(cluster)\r\n\r\nmodel <- kmeans(df, centers = 5, nstart = 20)\r\n\r\n\r\n\r\nLet’s break this down step-by-step:\r\nWe called the kmeans function from the cluster package\r\nKmeans function accepts several arguments, the first is to indicate\r\nwhat dataframe the analysis will be based on, in this case “df”\r\nThe other two arguments centers and\r\nnstart are core arguments in the k-means analysis.\r\ncenters = Determines the number of centroids in a given\r\ndataset. Centroids are imaginary or real locations representing the\r\ncenter of a cluster\r\nnstart = Specifies the number of random data sets used to\r\nrun the algorithm. Optionally, you can also explicitly determine how\r\nmany times this process runs by including a iter.max argument\r\n(default = 10). Andrea\r\nGrianti provides an excellent delineation between nstart\r\nand interations, both have similar but distinct meanings:\r\n\r\nif you want 3 clusters and you specify nstart = 10 it extracts 3 sets\r\nof data, 10 times, and for each of these times, the algorithm is run (up\r\nto iter.max = # of iterations) and the centroid setting that outputs the\r\nsmallest within-cluster sum of squares is then chosen as the result -\r\nAndrea Grianti -\r\n\r\nIn the above function, we arbitrarily determined the number\r\nof centers to be 5 and to sample our dataset using 5 hypothetical\r\ncentroids 20 times to find the setting with the minimum amount of\r\nvariance (i.e., within-cluster sum of squares). Because we did not\r\ndetermine the number of max interations, it defaults to 10 interations,\r\nthat is, it runs the sampling for the minimum within-cluster SS 10\r\ntimes.\r\nInterpreting K-Means\r\nClustering Output\r\nBelow is the output from our clustering analysis\r\n\r\nK-means clustering with 5 clusters of sizes 79, 39, 23, 23, 36\r\n\r\nCluster means:\r\n    Gender      Age Annual.Income..k.. Spending.Score..1.100.\r\n1 1.582278 43.08861           55.29114               49.56962\r\n2 1.538462 32.69231           86.53846               82.12821\r\n3 1.608696 45.21739           26.30435               20.91304\r\n4 1.608696 25.52174           26.30435               78.56522\r\n5 1.472222 40.66667           87.75000               17.58333\r\n\r\nClustering vector:\r\n  [1] 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4 3 4\r\n [33] 3 4 3 4 3 4 3 4 3 4 3 4 3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n [65] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n [97] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 5 2 1 2\r\n[129] 5 2 5 2 5 2 5 2 5 2 5 2 5 2 1 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2\r\n[161] 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2 5 2\r\n[193] 5 2 5 2 5 2 5 2\r\n\r\nWithin cluster sum of squares by cluster:\r\n[1] 30157.266 13982.051  8954.087  4627.739 17678.472\r\n (between_SS / total_SS =  75.6 %)\r\n\r\nAvailable components:\r\n\r\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \r\n[5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \r\n[9] \"ifault\"      \r\n\r\nThere are several important things to note in this output, let us\r\nbreak it down:\r\nThe first line informs us that 5 clusters were created, and the\r\nsize of each cluster (i.e., how many data points are included in the\r\ncluster) are provided. This gives us a general idea on how many\r\nsubgroups there are and what are their distribution in our data\r\nset.\r\nThe second output provides us with the means of each of our\r\nvariables according to our clusters. The gender column does not provide\r\nmeaningful data here because it does not make sense for someone to be\r\n1.60 of a gender. The reason why we obtained this value is because the\r\nmean is estimated from the centroid location.\r\nThe clustering vector provides us with a glimpse which data point\r\nbelongs to which cluster. This information will be useful later on when\r\nwe aggregate this vector to the main data set, so that we can identify\r\nwhich customer belongs to which cluster.\r\nThe last output is essential in understanding your clustering\r\naccuracy. Remember, the objective here is not to obtain a 100% accuracy.\r\nWhy? Because a 100% clustering accuracy means that all 200 of our data\r\npoints are neatly categorized into 200 clusters, which, as you can tell,\r\nis not particularly informative\r\nHere you can see our within sum of squares by cluster results in a\r\n75.6% accuracy, meaning, 75.6% of the variance is captured by the\r\nclustering algorithm, which is not bad, given that we randomly set\r\nk to be 5\r\nDetermining Optimal\r\nk Value: Elbow Method\r\nRecall that in the earlier section, we mention that there is a method\r\nto optimally calculate for the best number of clusters for your dataset?\r\nHere, we will explore this method, first, we create a custom function to\r\nplot the within sum of squares, iterating from 1 to n number of\r\ncentroids. We will not cover how to create a function here, but this tutorial\r\nprovides a good introduction to creating custom functions in R.\r\n\r\n\r\nwssplot <- function(data, num.centroid=15, seed=123){\r\n  wss <- (nrow(data) - 1)*sum(apply(data,2,var))\r\n  for (i in 2:num.centroid) {\r\n    set.seed(seed)\r\n    wss[i] <- sum(kmeans(data, centers = i)$withinss)}\r\n  plot(1:num.centroid, wss, type = \"b\", xlab = \"Number of groups\",\r\n       ylab = \"Sum of squares within a group\")}\r\n\r\nwssplot(df, num.centroid = 30)\r\n\r\n\r\n\r\n\r\nIn the above function, we set the maximum number of centroids to be\r\n30, that is, the function loops through the kmeans function by iterating\r\nthe number of centroids from 1 to 30. The within sum of squares results\r\nof each iteration is then taken and plotted.\r\nHere, you can see that though there was a blip in wss values between\r\nk = 6 and k = 7, it appears that the first initial\r\ndrop in wss values occured between k = 4 and k = 5.\r\nThis means that 5 clusters appear to be the optimal number for our data\r\nset.\r\nDetermining\r\nOptimal k Value: Silhoutte Method\r\nAnother way to determine the optimal k value, and have been\r\nargued to be a better approach than the Elbow method. To begin, let us\r\nload our required libraries and begin our Silhoutte (SI) analysis\r\n\r\n\r\nlibrary(factoextra)\r\n\r\ndf_sil <- silhouette(model$cluster, dist(df))\r\nfviz_silhouette(df_sil)\r\n\r\n\r\n  cluster size ave.sil.width\r\n1       1   79          0.37\r\n2       2   39          0.53\r\n3       3   23          0.42\r\n4       4   23          0.60\r\n5       5   36          0.43\r\n\r\n\r\nThe fviz_silhouette () function outputs a graph that provides us a\r\nSilhoutte Width value (Y-Axis). This value ranges from -1 to 1.\r\nEssentially, a positive SI value means that the data points within that\r\ncluster is well matched within its cluster and poorly matched with its\r\nneighbouring cluster. A negative SI value would mean that clustering\r\nconfiguration was inappropriate, perhaps either having too many or too\r\nfew clusters.\r\nHere you can see that all our SI analysis is not too bad, we our\r\nclusters average around 0.44 SI. We can try to rerun the analysis using\r\na higher k value. Based on the elbow method, it would seem that\r\nk = 7 may provide a better clustering fit\r\n\r\n  cluster size ave.sil.width\r\n1       1   35          0.40\r\n2       2   38          0.39\r\n3       3   45          0.44\r\n4       4   10          0.32\r\n5       5   22          0.58\r\n6       6   29          0.50\r\n7       7   21          0.42\r\n\r\n\r\nHowever, running the clustering algorithm with k = 7 does\r\nnot change the SI value at all, remaining at an average of 0.44 SI.\r\nFurthermore, it would appear that some data points in our clusters are\r\nin a worse position than if we remained k at 5. Hence, to\r\nreduce overfitting and to adjust for that negative SI data point, we\r\nwill remain with k at 5.\r\nAnalyzing Patterns in Our\r\nClusters\r\nAfter all that analytical work, we can finally start discovering\r\nunderlying patterns in our clusters! We first need a couple of other\r\npackages, let’s load them up and create some plots for our\r\ninterpretation\r\n\r\n\r\nlibrary(GGally)\r\nlibrary(plotly)\r\n\r\n\r\n\r\nRecall in the earlier section, we talked about the Clustering Vector?\r\nSince we have completed our clustering analysis, we will now aggregate\r\nthe vectors as factors back into our main dataframe. This will help us\r\nin plotting based on clusters. The clusters have to be aggregated back\r\nas factors, not as vectors or your plot will come out quite weird\r\n\r\n\r\n\r\nThe output of this cluster analysis is up for interpretation and\r\nhighly depends on what kind of information you require. Here are some\r\nnotable analysis that can be garnered from this graph\r\nCluster 3 and 4 is made up of young adults. Cluster 3 are\r\nslightly older young adults with high annual income and high spending\r\nscore. Cluster 4 are made up of younger young adults with low annual\r\nincome, but concerningly with a high spending score, indicating that\r\nthis age group may be spending above their means.\r\nClusters 1, 2 and 5 is made up of both young and older adults\r\nwith varying annual incomes and spending scores.\r\nImportantly, Cluster 2, which makes up the majority of our\r\ncustomers, have a moderate annual income and spend according to their\r\nmeans. This suggest that the the types of retail shops in this mall\r\nshould ideally target these audiences. Hence, based on this information,\r\nwe would suggest that the mall refrain from bringing in too many luxury\r\nbrands, given that the majority of their customers are from the\r\nmiddle-income bracket and are not high spenders.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-06-28T15:22:34+08:00",
    "input_file": {}
  }
]
