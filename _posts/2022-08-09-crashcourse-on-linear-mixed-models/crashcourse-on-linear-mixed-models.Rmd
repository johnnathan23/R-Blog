---
title: "Crash Course on Linear Mixed Models TEST"
description: |
  This post is intended as a crash course introduction on LMMs based on how I would have liked to learn the basics of LMMs if I were to learn it from scratch. I am by no means an expert on LMMs, and thus, some information given here might be inaccurate, but where possible, i will cite appropriate papers and books to help assist in further reading and learning
author:
  - name: Jonathan Choo
    Affiliation: University of Nottingham, Malaysia
date: 2022-08-09
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

This is a test
Welcome to a crash course on Linear Mixed Models (LMMs)! This post is based heavily on the introductory tutorial written by [Brown (2012)](https://journals.sagepub.com/doi/full/10.1177/2515245920960351), hence, a lot of the explanations here are paraphrased from the original paper, but mixed in with some additional information that may help in a clearer understanding. There are slight variations in the naming conventions of LMMs, you may encounter Linear Mixed Efffect Models, Mixed Models, Multi-level Models, rest assured, these all refer to the same concept that we will be going through today.

Before we proceed, this course assumes that the reader has:

1. Basics in Statistics in Psychology
  - Specifically Regression Analysis and Analysis of Variance (ANOVA)
2. Basics of R language
3. Basics of Data Structure and Organization

_If you are not knowledgeable in these areas, it is highly recommended that you brush up on these skills before attempting this course._

Have everything? Great! Let us start off with introducing what LMEs are

## LMEs: The What, the Why, and the How

### LMEs: The What
LMEs are essentially a regression analysis on steroids, providing the flexibility of traditional regression analysis, but allowing for more accurate model fitting compared to regular variance analysis like ANOVA. It achieves this flexibility by introducing two types of effects: **1) Fixed Effects** and **2) Random Effects** - hence the name "mixed model", as it includes both of these effects, rather than a traditional Fixed-Effects only model like ANOVAs.

### LMEs: The Why
Placeholder

### LMEs: The How
Understanding Fixed Effects and Random Effects is crucial in running an LME analysis. To understand what these effects are, [Freeman (2017)](http://mfviz.com/hierarchical-models/) provides an excellent overview along with a beautifully animated visualization on Fixed Effects and Random Effects.

In simple words, Fixed effects are usually predictors of interest; Random effects (inclusive of both random intercepts and random slopes) are predictors that exerts a varying influence on the outcome. Using the lme4 package, we can create a model that includes both of these effects. The code below exemplifies a typical mixed model 

```{r, echo=TRUE}
library(lme4)
```


```{r LME Example, eval=FALSE, echo=TRUE}

Example_Model <- lmer(Outcome ~  Fixed Effect + (1|Random Effect), data = Example_Data)

```

A Random Effect term is expressed as (1|X), it is here where you need to include vital information about random slopes and intercepts. The more random effects you have, and the more complicated the structure is, the more likely your model will become overfitted. Also, these random effects are usually higher-order grouping variables. Meaning, they supersede other experimental variables such as age, SES, Gender. For example, the two most common random effects are subjects and stimuli because these variables groups other experimental variables under them. Importantly, random effects must be a within-subject variable, that is, the variable must be the same in all blocks of your experiment. If a variable is different in each block, it cannot be a random effect. For example, if we wanted to include a by-stimuli random effect, we must ensure that the stimuli used is the same across all blocks. If different blocks use different types of stimuli, you cannot use that stimuli as a random effect. You may consider running an LME on a block-by-block basis, but this restricts your analysis.

Supposing we wanted to include both by-subject and by-stimuli random effects, how would it be modelled in R? Take a look at the code below:

```{r LME Example2, eval=FALSE, echo=TRUE}

Example_Model <- lmer(Outcome ~  Fixed Effect + (1|subject) + (1|stimuli), data = Example_Data)

```

#### Random Intercepts
To include a random intercept, we include a "1" (i.e. (1|)), indicating the intercept, within the random effect structure. This informs the model to allow for the intercept to vary for each new participant. This is important because as Freeman (2017) suggests, not all participants start off at the same level. Allowing for the intercept to vary for each participant allows for a better fitting model. This goes the same as with the stimuli. Not all stimuli are created equal, hence, allowing each stimuli to be evaluated at their own intercept creates a better fitted model. The intercept is usually the outcome of your model, why? Look at the regression equation below:

```{r Regression Equation, eval=FALSE, echo=TRUE}

Y = mX + C

```

The Y here indicates your intercept, which is the outcome of the model. The X here refers to the values of your X variable, whatever that may be. The *m* refers to the gradient of that particular X variable, and the *C* refers to the residual error.

So, if our outcome is RT, our random effect structure with random intercepts is basically asking the model to allow for every participants to "start off" at different RT values. In the case of Freeman (2017), their outcome was salary, hence, their by-subject random intercept tells the model to allow for every subject to start off at different salaries.

#### Random Slopes
To include a random slope, we first need to identify which of our variables of interest could have different effects across subjects. It might be easier to understand this by looking at our regression equation again. Specifically, random slopes are your *m* value. Your *m* value determines how "strong" and effect has over the outcome, a high value of *m* suggests that for every one value change in your X, it would result in a greater increase/decrease in the Y, and vice versa.

Once we have identified our variable, we can include a random slope in our random effect structure by placing the variable to the left of the pipe as shown in the code below:

```{r LME Example3, eval=FALSE, echo=TRUE}

Example_Model <- lmer(Outcome ~  Fixed Effect + 
                        (1 + Fixed Effect|subject) +
                        (1 + Fixed Effect|stimuli), data = Example_Data)

```

Here, we took the Fixed Effect and included it as a random slope, making the assumption that the effects of the Fixed Effect can change across both subject and stimuli. Of course, this must be theoretically justified, or your model may not make sense (even if significant)